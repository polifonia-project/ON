"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2854],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>f});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=l(a),u=o,f=d["".concat(c,".").concat(u)]||d[u]||m[u]||r;return a?n.createElement(f,i(i({ref:t},p),{},{components:a})):n.createElement(f,i({ref:t},p))}));function f(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:o,i[1]=s;for(var l=2;l<r;l++)i[l]=a[l];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},1369:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var n=a(7462),o=(a(7294),a(3905));const r={sidebar_position:1},i="Introduction",s={unversionedId:"cometa/intro",id:"cometa/intro",title:"Introduction",description:"Static Badge",source:"@site/docs/cometa/intro.md",sourceDirName:"cometa",slug:"/cometa/intro",permalink:"/ontology-network/docs/cometa/intro",draft:!1,editUrl:"ahttps://github.com/polifonia-project/ontology-network/tree/main/website/docs/cometa/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"CoMeta",permalink:"/ontology-network/docs/category/cometa"}},c={},l=[],p={toc:l},d="wrapper";function m(e){let{components:t,...a}=e;return(0,o.kt)(d,(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"pathname:///pylode/cometa.html"},(0,o.kt)("img",{parentName:"a",src:"https://img.shields.io/badge/pylode-online-purple?style=plastic",alt:"Static Badge"}))),(0,o.kt)("p",null,"An extension of ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/polifonia-project/musicmeta-ontology"},"Music Meta")," to describe the metadata of music ",(0,o.kt)("strong",{parentName:"p"},"co"),"llections, ",(0,o.kt)("strong",{parentName:"p"},"co"),"rpora, ",(0,o.kt)("strong",{parentName:"p"},"co"),"ntainers, or simply music datasets! Here, metadata is described at the collection-level (data curator, annotations provided, availability of audio music, etc.), and at the content-level, (e.g., the title, artist, release of each piece in a dataset). The design of CoMeta is informed by a survey of Music Information Retrieval datasets, which allowed for the categorisation of common fields."),(0,o.kt)("p",null,"The ontology designed to describe music datasets as containers of music-related data with specific characteristics and annotations."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Collection Information"),": the ontology captures information about the dataset as a whole, including the number of records (compositions or performances), genres, year of release, collection metadata (project investigator, university, etc.), and content metadata (specification document with track-level information like title, artist, release, MusicBrainz identifier). This also includes properties such as music media type (audio or symbolic), duration, audio format (MP3, WAV, FLAC), symbolic format (MIDI, MusicXML, MEI), and other additional media (audio features, rankings, etc.).")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Annotations"),": to represent the annotations provided within the dataset, which are crucial for MIR tasks. It would include various types of annotations contributed by domain experts (musicologists, composition teachers) or listeners, covering aspects like music structure, key, chord progressions, emotions, listening habits, etc.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Computational tasks"),": to define the different tasks that a dataset enables based on the available annotations. Examples in MIR include music emotion recognition, pattern extraction, cadence detection, etc. Together with the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/polifonia-project/music-algorithm-ontology"},"Music Algorithm")," ontology (its sibling ontology) it also allows to track the performance/accuracy of computational methods tested on each dataset.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Access and availability"),": to capture information regarding the accessibility of the dataset, including whether it is open, on-demand, or closed, and whether it can be accessed online or requires manual provisioning. It may also include details about an API if available.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"License/Copyright"),": to represent the licensing and copyright information associated with the dataset, ensuring compliance and proper attribution.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"References"),": to provide links to official websites and academic manuscripts describing the dataset and its collection process, facilitating proper citation and reference."))),(0,o.kt)("p",null,"By incorporating and supporting these requirements, the ontology would provide a structured representation of music datasets, their metadata, annotations, and interconnections. It would enable researchers and practitioners to explore, analyse, and utilize the datasets more effectively, promote interoperability, and facilitate the automatic discovery and extraction of knowledge from music-related data."))}m.isMDXComponent=!0}}]);